{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and deploying a Hugging Face model on SageMaker with ICD10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we install the required libraries from Hugging Face and AWS. Additionally, we make sure we have a compatible PyTorch version installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.7/site-packages (2.100.0)\n",
      "Requirement already satisfied: attrs<22,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.21.6)\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (1.24.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (21.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.48.0) (3.20.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.12 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.48.0) (1.27.12)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.20.21->sagemaker>=2.48.0) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.48.0) (2.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.48.0) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.48.0) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.48.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.48.0) (2.8.1)\n",
      "Requirement already satisfied: dill>=0.3.5.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess>=0.70.13 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.70.13)\n",
      "Requirement already satisfied: pox>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (0.3.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.5 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.48.0) (1.7.6.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.12->boto3<2.0,>=1.20.21->sagemaker>=2.48.0) (1.26.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==1.8.0 in /opt/conda/lib/python3.7/site-packages (1.8.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (0.0.19)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (2.28.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (21.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (0.3.5.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (1.5.0)\n",
      "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (0.70.13)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (2022.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (1.21.6)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (4.42.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets==1.8.0) (1.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (4.2.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets==1.8.0) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets==1.8.0) (2.4.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets==1.8.0) (1.26.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets==1.8.0) (2.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets==1.8.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->datasets==1.8.0) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets=='1.8.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Restart the kernel after installing the above packages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::552327042361:role/service-role/AmazonSageMaker-ExecutionRole-20201202T215511\n",
      "sagemaker bucket: sagemaker-ap-southeast-1-552327042361\n",
      "sagemaker session region: ap-southeast-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_to_input_file = \"ICD-10-GT-AAA.csv\"\n",
    "df = pd.read_csv(path_to_input_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar chol...</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholera, unspecified</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Typhoid fever, unspecified</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoid meningitis</td>\n",
       "      <td>A0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>Postprocedural testicular hypofunction</td>\n",
       "      <td>E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>Postprocedural adrenocortical (-medullary) hyp...</td>\n",
       "      <td>E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>Postprocedural hemorrhage and hematoma of an e...</td>\n",
       "      <td>E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>Postprocedural hemorrhage and hematoma of an e...</td>\n",
       "      <td>E8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>Other postprocedural endocrine and metabolic c...</td>\n",
       "      <td>E8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3589 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                dx_text label\n",
       "0     Cholera due to Vibrio cholerae 01, biovar chol...    A0\n",
       "1       Cholera due to Vibrio cholerae 01, biovar eltor    A0\n",
       "2                                  Cholera, unspecified    A0\n",
       "3                            Typhoid fever, unspecified    A0\n",
       "4                                    Typhoid meningitis    A0\n",
       "...                                                 ...   ...\n",
       "3584             Postprocedural testicular hypofunction    E8\n",
       "3585  Postprocedural adrenocortical (-medullary) hyp...    E8\n",
       "3586  Postprocedural hemorrhage and hematoma of an e...    E8\n",
       "3587  Postprocedural hemorrhage and hematoma of an e...    E8\n",
       "3588  Other postprocedural endocrine and metabolic c...    E8\n",
       "\n",
       "[3589 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "        \"A0\":0,\n",
    "        \"A1\":1,\n",
    "        \"A2\":2,\n",
    "        \"A3\":3,\n",
    "        \"A4\":4,\n",
    "        \"A5\":5,\n",
    "        \"A6\":6,\n",
    "        \"A7\":7,\n",
    "        \"A8\":8,\n",
    "        \"A9\":9,\n",
    "        \"B0\":10,\n",
    "        \"B1\":11,\n",
    "        \"B2\":12,\n",
    "        \"B3\":13,\n",
    "        \"B4\":14,\n",
    "        \"B5\":15,\n",
    "        \"B6\":16,\n",
    "        \"B7\":17,\n",
    "        \"B8\":18,\n",
    "        \"B9\":19,\n",
    "        \"C0\":20,\n",
    "        \"C1\":21,\n",
    "        \"C2\":22,\n",
    "        \"C3\":23,\n",
    "        \"C4\":24,\n",
    "        \"C5\":25,\n",
    "        \"C6\":26,\n",
    "        \"C7\":27,\n",
    "        \"C8\":28,\n",
    "        \"C9\":29,\n",
    "        \"D0\":30,\n",
    "        \"D1\":31,\n",
    "        \"D2\":32,\n",
    "        \"D3\":33,\n",
    "        \"D4\":34,\n",
    "        \"D5\":35,\n",
    "        \"D6\":36,\n",
    "        \"D7\":37,\n",
    "        \"D8\":38,\n",
    "        \"E0\":39,\n",
    "        \"E1\":40,\n",
    "        \"E2\":41,\n",
    "        \"E3\":42,\n",
    "        \"E4\":43,\n",
    "        \"E5\":44,\n",
    "        \"E6\":45,\n",
    "        \"E7\":46,\n",
    "        \"E8\":47,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].replace(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar chol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholera, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Typhoid fever, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoid meningitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>Postprocedural testicular hypofunction</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>Postprocedural adrenocortical (-medullary) hyp...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3586</th>\n",
       "      <td>Postprocedural hemorrhage and hematoma of an e...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3587</th>\n",
       "      <td>Postprocedural hemorrhage and hematoma of an e...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>Other postprocedural endocrine and metabolic c...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3589 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                dx_text  label\n",
       "0     Cholera due to Vibrio cholerae 01, biovar chol...      0\n",
       "1       Cholera due to Vibrio cholerae 01, biovar eltor      0\n",
       "2                                  Cholera, unspecified      0\n",
       "3                            Typhoid fever, unspecified      0\n",
       "4                                    Typhoid meningitis      0\n",
       "...                                                 ...    ...\n",
       "3584             Postprocedural testicular hypofunction     47\n",
       "3585  Postprocedural adrenocortical (-medullary) hyp...     47\n",
       "3586  Postprocedural hemorrhage and hematoma of an e...     47\n",
       "3587  Postprocedural hemorrhage and hematoma of an e...     47\n",
       "3588  Other postprocedural endocrine and metabolic c...     47\n",
       "\n",
       "[3589 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['split'] = np.random.randn(df.shape[0], 1)\n",
    "\n",
    "msk = np.random.rand(len(df)) <= 0.8\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop('split', axis=1)\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.drop('split', axis=1)\n",
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cholera, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Typhoid fever with heart involvement</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Typhoid pneumonia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Typhoid osteomyelitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Typhoid fever with other complications</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Paratyphoid fever B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Salmonella enteritis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Salmonella sepsis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Salmonella meningitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            dx_text  label\n",
       "1   Cholera due to Vibrio cholerae 01, biovar eltor      0\n",
       "2                              Cholera, unspecified      0\n",
       "5              Typhoid fever with heart involvement      0\n",
       "6                                 Typhoid pneumonia      0\n",
       "8                             Typhoid osteomyelitis      0\n",
       "9            Typhoid fever with other complications      0\n",
       "11                              Paratyphoid fever B      0\n",
       "14                             Salmonella enteritis      0\n",
       "15                                Salmonella sepsis      0\n",
       "17                            Salmonella meningitis      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar chol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Typhoid fever, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoid meningitis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Typhoid arthritis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Paratyphoid fever A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Paratyphoid fever C</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Paratyphoid fever, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Localized salmonella infection, unspecified</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Salmonella pneumonia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Salmonella pyelonephritis</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              dx_text  label\n",
       "0   Cholera due to Vibrio cholerae 01, biovar chol...      0\n",
       "3                          Typhoid fever, unspecified      0\n",
       "4                                  Typhoid meningitis      0\n",
       "7                                   Typhoid arthritis      0\n",
       "10                                Paratyphoid fever A      0\n",
       "12                                Paratyphoid fever C      0\n",
       "13                     Paratyphoid fever, unspecified      0\n",
       "16        Localized salmonella infection, unspecified      0\n",
       "18                               Salmonella pneumonia      0\n",
       "21                          Salmonella pyelonephritis      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('nih_train.csv', index=False)\n",
    "test.to_csv('nih_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to S3\n",
    "\n",
    "Upload the `dataset` files to the default bucket in Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset uploaded to: s3://sagemaker-ap-southeast-1-552327042361/NIH-ICD10/data/nih_train.csv\n",
      "test dataset uploaded to: s3://sagemaker-ap-southeast-1-552327042361/NIH-ICD10/data/nih_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "local_train_dataset = \"nih_train.csv\"\n",
    "local_test_dataset = \"nih_test.csv\"\n",
    "\n",
    "# s3 uris for datasets\n",
    "remote_train_dataset = f\"s3://{sess.default_bucket()}/NIH-ICD10/data\"\n",
    "remote_test_dataset = f\"s3://{sess.default_bucket()}/NIH-ICD10/data\"\n",
    "\n",
    "# upload datasets\n",
    "S3Uploader.upload(os.path.join(local_train_dataset),remote_train_dataset)\n",
    "S3Uploader.upload(os.path.join(local_test_dataset),remote_test_dataset)\n",
    "\n",
    "print(f\"train dataset uploaded to: {remote_train_dataset}/{local_train_dataset}\")\n",
    "print(f\"test dataset uploaded to: {remote_test_dataset}/{local_test_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script that performs fine tuning is located here: `training/scripts/train.py`. Navigate to the source code location and open the `train.py` file. You can also go through it's contents by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 32,               # batch size for training\n",
    "                 'eval_batch_size': 64,                # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                # learning rate used during training\n",
    "                 'model_id':'distilbert-base-uncased', # pre-trained model\n",
    "                 'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                 'train_file': local_train_dataset,    # training dataset\n",
    "                 'test_file': local_test_dataset,      # test dataset\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of supported models: https://huggingface.co/models?library=pytorch,transformers&sort=downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `metric_definition` dictionary that contains regex-based definitions that will be used to parse the job logs and extract metrics. You can read more about parsing the cloudwatch logs [here](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'eval_loss',               'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy',           'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1',                 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision',          'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'nih-icd10-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train2.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = 'scripts',      # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6',             # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    "    metric_definitions   = metric_definitions # the metrics regex definitions to extract logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-28 03:58:42 Starting - Starting the training job...ProfilerReport-1658980722: InProgress\n",
      "...\n",
      "2022-07-28 03:59:36 Starting - Preparing the instances for training.........\n",
      "2022-07-28 04:01:13 Downloading - Downloading input data...\n",
      "2022-07-28 04:01:36 Training - Downloading the training image.....................\n",
      "2022-07-28 04:05:05 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:08,147 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:08,175 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:08,184 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:08,592 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.1.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.7.25-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.1-py3-none-any.whl size=24939 sha256=1e0fa507b093d9eca8590a1d1623fe2d304fc64ddbc2fdbe4613d0134719ec5d\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/fc/6e/a34132fbbbd4a2822f51a26966c2fad1d9d8106374735367e1\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, nltk, absl-py, rouge-score\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.2.0 nltk-3.6.7 regex-2022.7.25 rouge-score-0.1.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:14,908 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 64,\n",
      "        \"fp16\": true,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_id\": \"distilbert-base-uncased\",\n",
      "        \"test_file\": \"nih_test.csv\",\n",
      "        \"train_batch_size\": 32,\n",
      "        \"train_file\": \"nih_train.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"nih-icd10-2022-07-28-03-58-39-2022-07-28-03-58-42-298\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-552327042361/nih-icd10-2022-07-28-03-58-39-2022-07-28-03-58-42-298/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train2\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train2.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"test_file\":\"nih_test.csv\",\"train_batch_size\":32,\"train_file\":\"nih_train.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train2.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train2\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-552327042361/nih-icd10-2022-07-28-03-58-39-2022-07-28-03-58-42-298/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"test_file\":\"nih_test.csv\",\"train_batch_size\":32,\"train_file\":\"nih_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"nih-icd10-2022-07-28-03-58-39-2022-07-28-03-58-42-298\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-552327042361/nih-icd10-2022-07-28-03-58-39-2022-07-28-03-58-42-298/source/sourcedir.tar.gz\",\"module_name\":\"train2\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train2.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"64\",\"--fp16\",\"True\",\"--learning_rate\",\"3e-05\",\"--model_id\",\"distilbert-base-uncased\",\"--test_file\",\"nih_test.csv\",\"--train_batch_size\",\"32\",\"--train_file\",\"nih_train.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=3e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=nih_test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=nih_train.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train2.py --epochs 1 --eval_batch_size 64 --fp16 True --learning_rate 3e-05 --model_id distilbert-base-uncased --test_file nih_test.csv --train_batch_size 32 --train_file nih_train.csv\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:20,716 - datasets.builder - WARNING - Using custom data configuration default-0fec675ddb337f7a\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-0fec675ddb337f7a/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-0fec675ddb337f7a/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:21,648 - datasets.builder - WARNING - Using custom data configuration default-f4bf72ff7d9bd86f\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-f4bf72ff7d9bd86f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-f4bf72ff7d9bd86f/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:22,534 - filelock - INFO - Lock 140217325005904 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:23,399 - filelock - INFO - Lock 140217325005904 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:24,262 - filelock - INFO - Lock 140214374050840 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:25,970 - filelock - INFO - Lock 140214374050840 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:26,837 - filelock - INFO - Lock 140214374050840 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:28,761 - filelock - INFO - Lock 140214374050840 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:31,353 - filelock - INFO - Lock 140214374050840 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:32,226 - filelock - INFO - Lock 140214374050840 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:32,969 - __main__ - INFO -  loaded train_dataset length is: 2849\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:32,969 - __main__ - INFO -  loaded test_dataset length is: 740\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:34,719 - filelock - INFO - Lock 140214374050896 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2022-07-28 04:05:40,704 - filelock - INFO - Lock 140214374050896 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:46.996 algo-1:38 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.152 algo-1:38 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.153 algo-1:38 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.153 algo-1:38 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.155 algo-1:38 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.155 algo-1:38 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.340 algo-1:38 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.340 algo-1:38 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.340 algo-1:38 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.340 algo-1:38 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.341 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.341 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.341 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.341 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.342 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.343 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.344 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.345 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.346 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.346 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.346 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.346 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.346 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.347 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.347 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.348 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.349 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.349 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.349 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.350 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.351 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.352 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.352 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.352 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.352 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.352 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.353 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.354 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.355 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.355 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.355 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.355 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.356 algo-1:38 INFO hook.py:591] name:pre_classifier.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.357 algo-1:38 INFO hook.py:591] name:pre_classifier.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.357 algo-1:38 INFO hook.py:591] name:classifier.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.357 algo-1:38 INFO hook.py:591] name:classifier.bias count_params:48\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.357 algo-1:38 INFO hook.py:593] Total Trainable Params: 66990384\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.357 algo-1:38 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-07-28 04:05:47.361 algo-1:38 INFO hook.py:488] Hook is writing from the hook with pid: 38\u001b[0m\n",
      "\u001b[34m{'eval_loss': 3.728576898574829, 'eval_accuracy': 0.11351351351351352, 'eval_f1': 0.11351351351351352, 'eval_precision': 0.11351351351351352, 'eval_recall': 0.11351351351351352, 'eval_runtime': 3.6713, 'eval_samples_per_second': 201.564, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 37.3369, 'train_samples_per_second': 2.41, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#0150 tables [00:00, ? tables/s]#015                            #015#0150 tables [00:00, ? tables/s]#015                            #015#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 335kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.10k/232k [00:00<00:11, 19.7kB/s]#015Downloading:  14%|█▍        | 32.8k/232k [00:00<00:07, 26.5kB/s]#015Downloading:  41%|████      | 94.2k/232k [00:00<00:03, 36.5kB/s]#015Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 50.7kB/s] #015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 276kB/s] \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading:   1%|          | 4.10k/466k [00:00<00:23, 19.7kB/s]#015Downloading:   8%|▊         | 36.9k/466k [00:00<00:16, 26.7kB/s]#015Downloading:  21%|██▏       | 99.3k/466k [00:00<00:09, 36.7kB/s]#015Downloading:  44%|████▍     | 206k/466k [00:00<00:05, 50.8kB/s] #015Downloading:  93%|█████████▎| 435k/466k [00:01<00:00, 71.2kB/s]#015Downloading: 100%|██████████| 466k/466k [00:01<00:00, 444kB/s] \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 17.1kB/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?ba/s]#015 33%|███▎      | 1/3 [00:00<00:00,  2.91ba/s]#015 67%|██████▋   | 2/3 [00:00<00:00,  3.57ba/s]#015100%|██████████| 3/3 [00:00<00:00,  5.25ba/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?ba/s]#015100%|██████████| 1/1 [00:00<00:00, 10.25ba/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.36M/268M [00:00<00:06, 43.6MB/s]#015Downloading:   3%|▎         | 8.83M/268M [00:00<00:05, 43.9MB/s]#015Downloading:   5%|▌         | 13.4M/268M [00:00<00:05, 44.5MB/s]#015Downloading:   7%|▋         | 18.2M/268M [00:00<00:05, 45.3MB/s]#015Downloading:   8%|▊         | 22.6M/268M [00:00<00:05, 45.0MB/s]#015Downloading:  10%|▉         | 26.5M/268M [00:00<00:05, 42.9MB/s]#015Downloading:  11%|█▏        | 30.3M/268M [00:00<00:06, 38.3MB/s]#015Downloading:  13%|█▎        | 34.6M/268M [00:00<00:05, 39.4MB/s]#015Downloading:  14%|█▍        | 38.3M/268M [00:00<00:06, 35.8MB/s]#015Downloading:  16%|█▌        | 43.0M/268M [00:01<00:05, 38.6MB/s]#015Downloading:  18%|█▊        | 47.4M/268M [00:01<00:05, 40.0MB/s]#015Downloading:  19%|█▉        | 52.2M/268M [00:01<00:05, 42.1MB/s]#015Downloading:  21%|██▏       | 57.0M/268M [00:01<00:04, 43.7MB/s]#015Downloading:  23%|██▎       | 61.6M/268M [00:01<00:04, 44.4MB/s]#015Downloading:  25%|██▍       | 66.4M/268M [00:01<00:04, 45.5MB/s]#015Downloading:  27%|██▋       | 71.3M/268M [00:01<00:04, 46.4MB/s]#015Downloading:  28%|██▊       | 76.1M/268M [00:01<00:04, 47.0MB/s]#015Downloading:  30%|███       | 81.0M/268M [00:01<00:03, 47.4MB/s]#015Downloading:  32%|███▏      | 85.8M/268M [00:01<00:03, 47.5MB/s]#015Downloading:  34%|███▍      | 90.6M/268M [00:02<00:03, 47.7MB/s]#015Downloading:  36%|███▌      | 95.4M/268M [00:02<00:03, 45.9MB/s]#015Downloading:  37%|███▋      | 100M/268M [00:02<00:03, 45.5MB/s] #015Downloading:  39%|███▉      | 105M/268M [00:02<00:03, 46.2MB/s]#015Downloading:  41%|████      | 110M/268M [00:02<00:03, 46.9MB/s]#015Downloading:  43%|████▎     | 114M/268M [00:02<00:03, 47.1MB/s]#015Downloading:  45%|████▍     | 119M/268M [00:02<00:03, 47.5MB/s]#015Downloading:  46%|████▋     | 124M/268M [00:02<00:03, 47.9MB/s]#015Downloading:  48%|████▊     | 129M/268M [00:02<00:02, 47.8MB/s]#015Downloading:  50%|████▉     | 134M/268M [00:02<00:02, 48.1MB/s]#015Downloading:  52%|█████▏    | 139M/268M [00:03<00:02, 45.8MB/s]#015Downloading:  53%|█████▎    | 143M/268M [00:03<00:02, 44.8MB/s]#015Downloading:  55%|█████▌    | 148M/268M [00:03<00:02, 43.1MB/s]#015Downloading:  57%|█████▋    | 152M/268M [00:03<00:02, 41.7MB/s]#015Downloading:  58%|█████▊    | 156M/268M [00:03<00:02, 41.3MB/s]#015Downloading:  60%|█████▉    | 161M/268M [00:03<00:02, 41.7MB/s]#015Downloading:  62%|██████▏   | 165M/268M [00:03<00:02, 43.8MB/s]#015Downloading:  64%|██████▎   | 170M/268M [00:03<00:02, 45.5MB/s]#015Downloading:  65%|██████▌   | 175M/268M [00:03<00:01, 46.6MB/s]#015Downloading:  67%|██████▋   | 180M/268M [00:04<00:01, 45.0MB/s]#015Downloading:  69%|██████▉   | 185M/268M [00:04<00:01, 45.6MB/s]#015Downloading:  71%|███████   | 190M/268M [00:04<00:01, 46.6MB/s]#015Downloading:  73%|███████▎  | 195M/268M [00:04<00:01, 47.5MB/s]#015Downloading:  74%|███████▍  | 199M/268M [00:04<00:01, 47.4MB/s]#015Downloading:  76%|███████▌  | 204M/268M [00:04<00:01, 46.6MB/s]#015Downloading:  78%|███████▊  | 209M/268M [00:04<00:01, 47.8MB/s]#015Downloading:  80%|███████▉  | 214M/268M [00:04<00:01, 46.6MB/s]#015Downloading:  82%|████████▏ | 219M/268M [00:04<00:01, 47.4MB/s]#015Downloading:  84%|████████▎ | 224M/268M [00:04<00:00, 48.3MB/s]#015Downloading:  85%|████████▌ | 229M/268M [00:05<00:00, 46.0MB/s]#015Downloading:  87%|████████▋ | 234M/268M [00:05<00:00, 43.2MB/s]#015Downloading:  89%|████████▉ | 239M/268M [00:05<00:00, 45.1MB/s]#015Downloading:  91%|█████████ | 243M/268M [00:05<00:00, 43.7MB/s]#015Downloading:  92%|█████████▏| 248M/268M [00:05<00:00, 42.6MB/s]#015Downloading:  94%|█████████▍| 253M/268M [00:05<00:00, 44.5MB/s]#015Downloading:  96%|█████████▌| 257M/268M [00:05<00:00, 44.8MB/s]#015Downloading:  98%|█████████▊| 262M/268M [00:05<00:00, 45.2MB/s]#015Downloading:  99%|█████████▉| 266M/268M [00:05<00:00, 45.7MB/s]#015Downloading: 100%|██████████| 268M/268M [00:05<00:00, 45.1MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/90 [00:00<?, ?it/s]#015  1%|          | 1/90 [00:02<03:03,  2.06s/it]#015  2%|▏         | 2/90 [00:02<02:15,  1.54s/it]#015  3%|▎         | 3/90 [00:02<01:42,  1.17s/it]#015  4%|▍         | 4/90 [00:03<01:18,  1.09it/s]#015  6%|▌         | 5/90 [00:03<01:02,  1.36it/s]#015  7%|▋         | 6/90 [00:03<00:51,  1.64it/s]#015  8%|▊         | 7/90 [00:03<00:43,  1.92it/s]#015  9%|▉         | 8/90 [00:04<00:37,  2.17it/s]#015 10%|█         | 9/90 [00:04<00:34,  2.35it/s]#015 11%|█         | 10/90 [00:04<00:32,  2.49it/s]#015 12%|█▏        | 11/90 [00:05<00:30,  2.59it/s]#015 13%|█▎        | 12/90 [00:05<00:28,  2.70it/s]#015 14%|█▍        | 13/90 [00:05<00:27,  2.82it/s]#015 16%|█▌        | 14/90 [00:06<00:25,  2.93it/s]#015 17%|█▋        | 15/90 [00:06<00:26,  2.85it/s]#015 18%|█▊        | 16/90 [00:06<00:25,  2.94it/s]#015 19%|█▉        | 17/90 [00:07<00:24,  2.99it/s]#015 20%|██        | 18/90 [00:07<00:23,  3.03it/s]#015 21%|██        | 19/90 [00:07<00:23,  3.07it/s]#015 22%|██▏       | 20/90 [00:08<00:22,  3.09it/s]#015 23%|██▎       | 21/90 [00:08<00:22,  3.12it/s]#015 24%|██▍       | 22/90 [00:08<00:21,  3.11it/s]#015 26%|██▌       | 23/90 [00:09<00:21,  3.13it/s]#015 27%|██▋       | 24/90 [00:09<00:21,  3.12it/s]#015 28%|██▊       | 25/90 [00:09<00:20,  3.11it/s]#015 29%|██▉       | 26/90 [00:10<00:20,  3.07it/s]#015 30%|███       | 27/90 [00:10<00:20,  3.06it/s]#015 31%|███       | 28/90 [00:10<00:20,  3.05it/s]#015 32%|███▏      | 29/90 [00:11<00:19,  3.06it/s]#015 33%|███▎      | 30/90 [00:11<00:19,  3.08it/s]#015 34%|███▍      | 31/90 [00:11<00:18,  3.11it/s]#015 36%|███▌      | 32/90 [00:12<00:18,  3.13it/s]#015 37%|███▋      | 33/90 [00:12<00:18,  3.14it/s]#015 38%|███▊      | 34/90 [00:12<00:17,  3.16it/s]#015 39%|███▉      | 35/90 [00:13<00:17,  3.13it/s]#015 40%|████      | 36/90 [00:13<00:17,  3.13it/s]#015 41%|████      | 37/90 [00:13<00:16,  3.13it/s]#015 42%|████▏     | 38/90 [00:14<00:16,  3.16it/s]#015 43%|████▎     | 39/90 [00:14<00:16,  3.13it/s]#015 44%|████▍     | 40/90 [00:14<00:16,  3.09it/s]#015 46%|████▌     | 41/90 [00:15<00:16,  2.98it/s]#015 47%|████▋     | 42/90 [00:15<00:16,  2.94it/s]#015 48%|████▊     | 43/90 [00:15<00:15,  2.95it/s]#015 49%|████▉     | 44/90 [00:16<00:15,  2.98it/s]#015 50%|█████     | 45/90 [00:16<00:14,  3.03it/s]#015 51%|█████     | 46/90 [00:16<00:14,  3.04it/s]#015 52%|█████▏    | 47/90 [00:17<00:13,  3.08it/s]#015 53%|█████▎    | 48/90 [00:17<00:13,  3.09it/s]#015 54%|█████▍    | 49/90 [00:17<00:13,  3.11it/s]#015 56%|█████▌    | 50/90 [00:17<00:12,  3.09it/s]#015 57%|█████▋    | 51/90 [00:18<00:12,  3.07it/s]#015 58%|█████▊    | 52/90 [00:18<00:12,  3.01it/s]#015 59%|█████▉    | 53/90 [00:19<00:12,  2.95it/s]#015 60%|██████    | 54/90 [00:19<00:12,  3.00it/s]#015 61%|██████    | 55/90 [00:19<00:11,  2.99it/s]#015 62%|██████▏   | 56/90 [00:20<00:11,  2.95it/s]#015 63%|██████▎   | 57/90 [00:20<00:11,  2.89it/s]#015 64%|██████▍   | 58/90 [00:20<00:11,  2.88it/s]#015 66%|██████▌   | 59/90 [00:21<00:10,  2.93it/s]#015 67%|██████▋   | 60/90 [00:21<00:10,  2.97it/s]#015 68%|██████▊   | 61/90 [00:21<00:09,  3.01it/s]#015 69%|██████▉   | 62/90 [00:22<00:09,  3.03it/s]#015 70%|███████   | 63/90 [00:22<00:08,  3.06it/s]#015 71%|███████   | 64/90 [00:22<00:08,  3.09it/s]#015 72%|███████▏  | 65/90 [00:22<00:08,  3.08it/s]#015 73%|███████▎  | 66/90 [00:23<00:07,  3.06it/s]#015 74%|███████▍  | 67/90 [00:23<00:07,  3.08it/s]#015 76%|███████▌  | 68/90 [00:23<00:07,  3.12it/s]#015 77%|███████▋  | 69/90 [00:24<00:06,  3.11it/s]#015 78%|███████▊  | 70/90 [00:24<00:06,  3.12it/s]#015 79%|███████▉  | 71/90 [00:24<00:06,  3.12it/s]#015 80%|████████  | 72/90 [00:25<00:05,  3.08it/s]#015 81%|████████  | 73/90 [00:25<00:05,  3.05it/s]#015 82%|████████▏ | 74/90 [00:25<00:05,  3.06it/s]#015 83%|████████▎ | 75/90 [00:26<00:04,  3.05it/s]#015 84%|████████▍ | 76/90 [00:26<00:04,  3.06it/s]#015 86%|████████▌ | 77/90 [00:26<00:04,  3.07it/s]#015 87%|████████▋ | 78/90 [00:27<00:03,  3.10it/s]#015 88%|████████▊ | 79/90 [00:27<00:03,  3.09it/s]#015 89%|████████▉ | 80/90 [00:27<00:03,  3.08it/s]#015 90%|█████████ | 81/90 [00:28<00:02,  3.07it/s]#015 91%|█████████ | 82/90 [00:28<00:02,  2.95it/s]#015 92%|█████████▏| 83/90 [00:28<00:02,  2.94it/s]#015 93%|█████████▎| 84/90 [00:29<00:01,  3.00it/s]#015 94%|█████████▍| 85/90 [00:29<00:01,  3.01it/s]#015 96%|█████████▌| 86/90 [00:29<00:01,  3.03it/s]#015 97%|█████████▋| 87/90 [00:30<00:01,  3.00it/s]#015 98%|█████████▊| 88/90 [00:30<00:00,  3.04it/s]#015 99%|█████████▉| 89/90 [00:30<00:00,  3.08it/s]#015100%|██████████| 90/90 [00:30<00:00,  3.75it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/12 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 17%|█▋        | 2/12 [00:00<00:01,  6.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 25%|██▌       | 3/12 [00:00<00:01,  4.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 33%|███▎      | 4/12 [00:00<00:01,  4.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m2022-07-28 04:06:29,803 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015 42%|████▏     | 5/12 [00:01<00:01,  3.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 50%|█████     | 6/12 [00:01<00:01,  3.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 58%|█████▊    | 7/12 [00:01<00:01,  3.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 8/12 [00:02<00:01,  3.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|███████▌  | 9/12 [00:02<00:00,  3.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 83%|████████▎ | 10/12 [00:02<00:00,  3.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 92%|█████████▏| 11/12 [00:03<00:00,  3.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 12/12 [00:03<00:00,  3.64it/s]#033[A#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015100%|██████████| 90/90 [00:34<00:00,  3.75it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 12/12 [00:03<00:00,  3.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A#015                                               #015#015100%|██████████| 90/90 [00:37<00:00,  3.75it/s]#015100%|██████████| 90/90 [00:37<00:00,  2.41it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/12 [00:00<?, ?it/s]#015 17%|█▋        | 2/12 [00:00<00:01,  6.34it/s]#015 25%|██▌       | 3/12 [00:00<00:01,  4.87it/s]#015 33%|███▎      | 4/12 [00:00<00:01,  4.20it/s]#015 42%|████▏     | 5/12 [00:01<00:01,  3.84it/s]#015 50%|█████     | 6/12 [00:01<00:01,  3.62it/s]#015 58%|█████▊    | 7/12 [00:01<00:01,  3.46it/s]#015 67%|██████▋   | 8/12 [00:02<00:01,  3.38it/s]#015 75%|███████▌  | 9/12 [00:02<00:00,  3.32it/s]#015 83%|████████▎ | 10/12 [00:02<00:00,  3.28it/s]#015 92%|█████████▏| 11/12 [00:03<00:00,  3.25it/s]#015100%|██████████| 12/12 [00:03<00:00,  3.69it/s]#015100%|██████████| 12/12 [00:03<00:00,  3.60it/s]\u001b[0m\n",
      "\n",
      "2022-07-28 04:06:37 Uploading - Uploading generated training model\n",
      "2022-07-28 04:08:46 Completed - Training job completed\n",
      "Training seconds: 453\n",
      "Billable seconds: 453\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "training_data = {\n",
    "    'train': remote_train_dataset,\n",
    "    'test': remote_test_dataset\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(training_data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "training_job_name = huggingface_estimator.latest_training_job.name\n",
    "print(f\"Training jobname: {training_job_name}\")\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"I love using the new Inference DLC.\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-1:492261229750:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
